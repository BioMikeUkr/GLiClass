{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score, precision_recall_fscore_support, accuracy_score\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "from gliclass import GLiClassModel, ZeroShotClassificationPipeline\n",
    "from gliclass.data_processing import GLiClassDataset, DataCollatorWithPadding\n",
    "from gliclass.training import TrainingArguments, Trainer\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gliclass_predictions(pipeline, test_texts, classes, batch_size=8):\n",
    "    results = pipeline(test_texts, classes, batch_size=batch_size)#, labels_chunk_size=1)\n",
    "    predicts = [result[0]['label'] for result in results]\n",
    "    return predicts\n",
    "\n",
    "def evaluate(predicts, true_labels):\n",
    "    micro = f1_score(true_labels, predicts, average=\"micro\")\n",
    "    macro = f1_score(true_labels, predicts, average=\"macro\")\n",
    "    weighted = f1_score(true_labels, predicts, average=\"weighted\")\n",
    "    return {\"micro\": micro, \"macro\": macro, \"weighted\": weighted}\n",
    "\n",
    "def get_train_dataset(dataset, N, label_column='label'):\n",
    "    ids = []\n",
    "    label2count = {}\n",
    "    train_dataset = dataset.shuffle(seed=41)\n",
    "    for id, example in enumerate(train_dataset):\n",
    "        if example[label_column] not in label2count:\n",
    "            label2count[example[label_column]]=1\n",
    "        elif label2count[example[label_column]]>=N:\n",
    "            continue\n",
    "        else:\n",
    "            label2count[example[label_column]]+=1\n",
    "        ids.append(id)\n",
    "    return train_dataset.select(ids)\n",
    "\n",
    "def prepare_dataset(dataset, classes = None, text_column = 'text', label_column = \"label\", split=None):\n",
    "    if 'test' in dataset:\n",
    "        test_dataset = dataset['test']\n",
    "    elif isinstance(dataset, Dataset):\n",
    "        test_dataset = dataset\n",
    "    else:\n",
    "        test_dataset = dataset['train']\n",
    "    \n",
    "    if classes is None:\n",
    "        classes = test_dataset.features[label_column].names\n",
    "        if split is not None:\n",
    "            classes = [' '.join(class_.split(split)) for class_ in classes]\n",
    "\n",
    "    texts = test_dataset[text_column]\n",
    "\n",
    "    true_labels = test_dataset[label_column]\n",
    "\n",
    "    print(classes)\n",
    "    if type(test_dataset[label_column][0]) == int:\n",
    "        true_labels = [classes[label] for label in true_labels]\n",
    "\n",
    "    return texts, classes, true_labels\n",
    "\n",
    "\n",
    "def prepare_dataset_for_training(train_dataset, classes, text_column='text', label_column='label'):\n",
    "    id2class = {id: class_ for id, class_ in enumerate(classes)}\n",
    "    dataset = []\n",
    "    for example in train_dataset:\n",
    "        label = example[label_column]\n",
    "        if type(label)==int:\n",
    "            label = id2class[label]\n",
    "        item = {'text': example[text_column], 'all_labels': classes, 'true_labels': [label]}\n",
    "        dataset.append(item)\n",
    "    random.shuffle(dataset)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = load_dataset('dair-ai/emotion')\n",
    "\n",
    "train_data = get_train_dataset(emotions['train'], N=64)\n",
    "\n",
    "test_texts, classes, true_labels = prepare_dataset(emotions)\n",
    "\n",
    "train_data = prepare_dataset_for_training(train_data, classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_news = load_dataset('ag_news')\n",
    "\n",
    "train_data = get_train_dataset(ag_news['train'], N=64)\n",
    "\n",
    "test_texts, classes, true_labels = prepare_dataset(ag_news)\n",
    "\n",
    "train_data = prepare_dataset_for_training(train_data, classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst5 = load_dataset('SetFit/sst5')\n",
    "\n",
    "train_data = get_train_dataset(sst5['train'], N=64)\n",
    "\n",
    "classes = ['very negative', 'negative', 'neutral', 'positive', 'very positive']\n",
    "\n",
    "test_texts, classes, true_labels = prepare_dataset(sst5, classes=classes)\n",
    "\n",
    "train_data = prepare_dataset_for_training(train_data, classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banking = load_dataset('PolyAI/banking77')\n",
    "\n",
    "train_data = get_train_dataset(banking['train'], N=32)\n",
    "\n",
    "test_texts, classes, true_labels = prepare_dataset(banking)\n",
    "\n",
    "train_data = prepare_dataset_for_training(train_data, classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "massive = load_dataset(\"AmazonScience/massive\", \"en-US\")\n",
    "\n",
    "train_data = get_train_dataset(massive['train'], N=32, label_column='intent')\n",
    "\n",
    "test_texts, classes, true_labels = prepare_dataset(massive, text_column='utt', label_column='intent')\n",
    "\n",
    "train_data = prepare_dataset_for_training(train_data, classes,  text_column='utt', label_column='intent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'knowledgator/gliclass-base-v1.0'\n",
    "\n",
    "model = GLiClassModel.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1024\n",
    "problem_type = \"multi_label_classification\"\n",
    "architecture_type = model.config.architecture_type\n",
    "prompt_first = model.config.prompt_first\n",
    "\n",
    "train_dataset = GLiClassDataset(train_data, tokenizer, max_length, problem_type, architecture_type, prompt_first)\n",
    "test_dataset = GLiClassDataset(train_data[:int(len(train_data)*0.1)], tokenizer, max_length, problem_type, architecture_type, prompt_first)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(device=device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='models/test',\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    others_lr=1e-5,\n",
    "    others_weight_decay=0.01,\n",
    "    lr_scheduler_type='linear',\n",
    "    warmup_ratio=0.0,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_steps = 1000,\n",
    "    save_total_limit=10,\n",
    "    dataloader_num_workers=8,\n",
    "    logging_steps=10,\n",
    "    use_cpu = False,\n",
    "    report_to=\"none\",\n",
    "    fp16=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ZeroShotClassificationPipeline(model, tokenizer, classification_type='single-label', device='cuda:0')\n",
    "\n",
    "predicts = get_gliclass_predictions(pipeline, test_texts, classes, batch_size=8)\n",
    "\n",
    "results = evaluate(predicts, true_labels)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
